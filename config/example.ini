[Data]
encoder = 'lstm'
feat = ['char']
data_multiply = 5

[Network]
normalize_emb = True
n_embed = 100
n_char_embed = 50
n_feat_embed = 100
embed_dropout = .33
n_lstm_hidden = 400
n_lstm_layers = 3
encoder_dropout = .33
n_arc_mlp = 500
n_rel_mlp = 100
mlp_dropout = .33

[Optimizer]
lr = 2e-3
mu = .9
nu = .9
eps = 1e-12
weight_decay = 0
clip = 5.0
min_freq = 2
fix_len = 20
decay = .75
decay_steps = 5000
update_steps = 1

[SemiSupervised]
load_from_saved = 'log/init_30percent'
contrastive_training = True
contrastive_loss_weight = ['0@0','1@20']
contrastive_algo = 'vat'
vat_mode = 'ftop7'
contrastive_distance = 'ce'
vat_scale_type = 'token'
vat_distribution = 'uniform'
vat_xi = 0.5
vat_eps = 0.1
vat_filter_mode = 'none'
vat_interpolation = 0.
vat_detach_p = True

[Train]
epochs = 100